# ğŸ§  AI - FashionMNIST Feedforward Neural Network

## ğŸ“Œ Overview
This repository contains an implementation of a **Feedforward Neural Network (FNN)** trained on the **FashionMNIST** dataset. The project explores the impact of different **activation functions** and **optimizers** on model performance and further improves training using **learning rate scheduling and additional epochs**.

## ğŸš€ Features
- **Custom Feedforward Neural Network** with 3 hidden layers.
- **Experimentation** with:
  - Three **activation functions**: ReLU, Sigmoid, and Tanh.
  - Two **optimizers**: SGD and Adam.
- **Performance Improvement** using:
  - **Learning rate scheduling (StepLR)**
  - **Extended training epochs**

## ğŸ“‚ Files and Structure
- `main.ipynb` - Jupyter Notebook containing code, training, and experiments.
- `Results Report.docx` - A concise report analyzing the findings and results.
- `README.md` - This documentation file.
- `requirements.txt` - Python dependencies required for running the code.

## ğŸ”§ Installation & Setup
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/FashionMNIST-FNN.git
cd FashionMNIST-FNN
```

### 2ï¸âƒ£ Set Up Virtual Environment
```bash
# Windows (Run PowerShell as Administrator)
Set-ExecutionPolicy Unrestricted -Scope Process    

# Create Virtual Environment
python -m venv venv         
venv\Scripts\activate        # Windows
# OR
source venv/bin/activate     # MacOS/Linux

# Install dependencies
pip install -r requirements.txt
```

### 3ï¸âƒ£ GPU Setup (Optional)
If CUDA is not detected but a GPU is available:
```bash
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## ğŸ“Š Experiment Results

### **(b) Activation Function & Optimizer Comparison**
| Activation | Optimizer | Test Accuracy (%) | Rank |
|------------|------------|------------------|------|
| **ReLU**   | **SGD**    | 79.26            | 5    |
| **ReLU**   | **Adam**   | **87.08** ğŸ†     | 1    |
| **Sigmoid**| **SGD**    | 10.00 âŒ         | 6    |
| **Sigmoid**| **Adam**   | 86.27            | 3    |
| **Tanh**   | **SGD**    | 81.83            | 4    |
| **Tanh**   | **Adam**   | 87.06            | 2    |

ğŸ”¹ **Best Model:** **ReLU + Adam (87.08%)**  
ğŸ”¹ **Worst Model:** **Sigmoid + SGD (10.00%)** (Vanishing gradient problem)  

### **(c) Improving the Best Model**
| Activation | Optimizer | Test Accuracy (%) | Epochs |
|------------|------------|------------------|--------|
| **ReLU**   | **SGD**    | 88.15            | 5      |
| **ReLU**   | **SGD**    | **89.53** ğŸš€     | 15     |

ğŸ”¹ **By increasing epochs and applying learning rate scheduling, we improved accuracy from 87.08% â†’ 89.53%** ğŸ¯  

## ğŸ“œ Key Learnings
âœ… **ReLU performs best**, avoiding vanishing gradient issues.  
âœ… **Adam optimizer is more effective** than SGD for this dataset.  
âœ… **Training longer (more epochs) and using a Learning Rate Scheduler significantly improves accuracy.**  

## ğŸ“Œ Usage
To run the experiments and train the model:
```bash
jupyter notebook main.ipynb
```

## ğŸ‘¨â€ğŸ’» Author
**Ong Jung Yi**
ğŸ“§ Contact: [ong.jung.yi.1503@gmail.com]
