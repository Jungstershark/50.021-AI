{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a Feed-Forward Neural Network\n",
    "\n",
    "### Task (a): Constructing a Custom Feed-Forward Neural Network\n",
    "\n",
    "In this section, we develop a feed-forward neural network (FNN) with three hidden layers using PyTorch. The network follows the given specifications:\n",
    "\n",
    "- **Input Layer**: Accepts a flattened 28×28 FashionMNIST image (784 input features).\n",
    "- **Hidden Layers**:\n",
    "  - **Layer 1**: 512 neurons\n",
    "  - **Layer 2**: 256 neurons\n",
    "  - **Layer 3**: 128 neurons\n",
    "- **Output Layer**: 10 neurons (corresponding to 10 FashionMNIST classes).\n",
    "- **Activation Function**: Applied after each hidden layer.\n",
    "- **Dropout**: Used to improve generalization and prevent overfitting.\n",
    "\n",
    "### Implementation Steps:\n",
    "1. **Define the `CustomFeedForwardNN` class**:\n",
    "   - Uses `nn.ModuleList` to handle multiple hidden layers dynamically.\n",
    "   - Applies the activation function and dropout after each hidden layer.\n",
    "   - Uses a final linear layer to produce class logits.\n",
    "\n",
    "2. **Instantiate the Model**:\n",
    "   - Input size is 784 (28×28).\n",
    "   - Uses ReLU as the activation function (which can be varied later).\n",
    "   - Applies a dropout rate of 0.2.\n",
    "\n",
    "3. **Verify Model Summary**:\n",
    "   - Uses `torchinfo.summary` to inspect the architecture and parameter count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomFeedForwardNN(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, num_classes, hidden_dims, dropout, activation_fn):\n",
    "    super().__init__()\n",
    "\n",
    "    # Ensure that hidden_dims is a non-empty list\n",
    "    assert isinstance(hidden_dims, list) and len(hidden_dims) > 0\n",
    "\n",
    "    # Initialize a ModuleList to store the hidden layers\n",
    "    self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "    # Input layer to first hidden layer\n",
    "    self.hidden_layers.append(nn.Linear(input_size, hidden_dims[0]))\n",
    "\n",
    "    # Subsequent hidden layers\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "      self.hidden_layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "\n",
    "    # Set up the nonlinearity to use between layers.\n",
    "    self.nonlinearity = activation_fn\n",
    "\n",
    "    # Set up the dropout layer.\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Set up the final transform to a distribution over classes.\n",
    "    self.output_projection = nn.Linear(hidden_dims[-1], num_classes)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # Apply the hidden layers, nonlinearity, and dropout.\n",
    "    for hidden_layer in self.hidden_layers:\n",
    "      x = hidden_layer(x)\n",
    "      x = self.nonlinearity(x)\n",
    "      x = self.dropout(x)\n",
    "      \n",
    "    # Output logits\n",
    "    out = self.output_projection(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the model parameters\n",
    "input_size = 28 * 28  # For 28x28 pixel images\n",
    "hidden_dims = [512, 256, 128]\n",
    "num_classes = 10  # Number of output classes in FashionMNIST\n",
    "dropout_rate = 0.2\n",
    "activation_fn = nn.ReLU()  # Example activation function\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomFeedForwardNN(input_size, num_classes, hidden_dims, dropout_rate, activation_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CustomFeedForwardNN                      [1, 10]                   --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Linear: 2-1                       [1, 512]                  401,920\n",
       "├─ReLU: 1-2                              [1, 512]                  --\n",
       "├─Dropout: 1-3                           [1, 512]                  --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Linear: 2-2                       [1, 256]                  131,328\n",
       "├─ReLU: 1-5                              [1, 256]                  --\n",
       "├─Dropout: 1-6                           [1, 256]                  --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Linear: 2-3                       [1, 128]                  32,896\n",
       "├─ReLU: 1-8                              [1, 128]                  --\n",
       "├─Dropout: 1-9                           [1, 128]                  --\n",
       "├─Linear: 1-10                           [1, 10]                   1,290\n",
       "==========================================================================================\n",
       "Total params: 567,434\n",
       "Trainable params: 567,434\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.57\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 2.27\n",
       "Estimated Total Size (MB): 2.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification of model information\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1,input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Activation Functions and Optimizers\n",
    "\n",
    "### Task (b): Evaluating Model Performance with Different Configurations\n",
    "\n",
    "In this section, we experiment with various activation functions and optimizers to evaluate their impact on model performance. The goal is to determine which combination yields the best test accuracy on the FashionMNIST dataset.\n",
    "\n",
    "### Experimental Setup:\n",
    "1. **Dataset Loading**:\n",
    "   - The **FashionMNIST** dataset is loaded and transformed into tensors.\n",
    "   - Training and testing data are handled using PyTorch `DataLoader` with a batch size of **64**.\n",
    "\n",
    "2. **Activation Functions Tested**:\n",
    "   - **ReLU** (Rectified Linear Unit)\n",
    "   - **Sigmoid**\n",
    "   - **Tanh**\n",
    "\n",
    "3. **Optimizers Tested**:\n",
    "   - **SGD (Stochastic Gradient Descent)** with momentum\n",
    "   - **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "4. **Training and Evaluation**:\n",
    "   - The model is trained for **5 epochs** using a **cross-entropy loss function**.\n",
    "   - Training accuracy and loss are monitored per epoch.\n",
    "   - After training, the model is evaluated on the test set, and the test accuracy is recorded.\n",
    "\n",
    "5. **Result Storage**:\n",
    "   - The results (activation function, optimizer type, and test accuracy) are stored in a Pandas DataFrame for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms \n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "### Loading MINST data\n",
    "\n",
    "# Define Transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = FashionMNIST(root='./torchvision-data',\n",
    "                             train=True,\n",
    "                             transform=transform,\n",
    "                             download=True)\n",
    "\n",
    "test_dataset = FashionMNIST(root='./torchvision-data', \n",
    "                            train=False,\n",
    "                            transform=transform,\n",
    "                            download=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Simple GPU check\n",
    "using_GPU = torch.cuda.is_available()\n",
    "print(\"Using GPU?\", using_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7628,  1.3272, -0.0509],\n",
      "        [ 0.0763,  0.2536,  0.2860]])\n",
      "ReLU()\n",
      "tensor([[0.7628, 1.3272, 0.0000],\n",
      "        [0.0763, 0.2536, 0.2860]])\n",
      "Tanh()\n",
      "tensor([[ 0.6427,  0.8686, -0.0509],\n",
      "        [ 0.0761,  0.2483,  0.2784]])\n",
      "Sigmoid()\n",
      "tensor([[0.6820, 0.7904, 0.4873],\n",
      "        [0.5191, 0.5631, 0.5710]])\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "data = torch.randn(2, 3)\n",
    "print(data)\n",
    "\n",
    "# Nonlinearities are layers too!\n",
    "relu = nn.ReLU()\n",
    "print(relu)\n",
    "print(relu(data))\n",
    "\n",
    "tanh = nn.Tanh()\n",
    "print(tanh)\n",
    "print(tanh(data))\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(sigmoid)\n",
    "print(sigmoid(data))\n",
    "\n",
    "\n",
    "activations = [nn.ReLU(), nn.Sigmoid(), nn.Tanh()]\n",
    "optimizers = [\"SGD\", \"Adam\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_and_evaluate_model(activation_fn, optimizer_type, learning_rate=0.001, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model using the given activation function and optimizer.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = CustomFeedForwardNN(input_size=784, num_classes=10, hidden_dims=[512, 256, 128], dropout=0.2, activation_fn=activation_fn)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss Function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer Selection\n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(images.shape[0], -1).to(device)  # Flatten images\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Evaluate Model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(images.shape[0], -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.8716, Accuracy: 33.16%\n",
      "Epoch [2/5], Loss: 0.9570, Accuracy: 62.92%\n",
      "Epoch [3/5], Loss: 0.7659, Accuracy: 71.23%\n",
      "Epoch [4/5], Loss: 0.6811, Accuracy: 75.46%\n",
      "Epoch [5/5], Loss: 0.6209, Accuracy: 78.14%\n",
      "Test Accuracy: 79.55%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79.55"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "activations = {\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    \"Sigmoid\": nn.Sigmoid(),\n",
    "    \"Tanh\": nn.Tanh()\n",
    "}\n",
    "\n",
    "optimizers = [\"SGD\", \"Adam\"]\n",
    "\n",
    "# This line is to test on a single configuration. Before running on all combinations\n",
    "# train_and_evaluate_model(activation_fn=nn.ReLU(), optimizer_type=\"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Activation\", \"Optimizer\", \"Test Accuracy (%)\"])\n",
    "\n",
    "for act_name, act_fn in activations.items():\n",
    "    for opt in optimizers:\n",
    "        print(f\"\\n===== Running: Activation={act_name}, Optimizer={opt} =====\")\n",
    "        acc = train_and_evaluate_model(activation_fn=act_fn, optimizer_type=opt)\n",
    "\n",
    "        # Append results to DataFrame\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            \"Activation\": [act_name],\n",
    "            \"Optimizer\": [opt],\n",
    "            \"Test Accuracy (%)\": [acc]\n",
    "        })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_with_scheduler(activation_fn, optimizer_type, learning_rate=0.001, num_epochs=15):\n",
    "    \"\"\"\n",
    "    Train and evaluate the improved model with learning rate scheduling.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CustomFeedForwardNN(input_size=784, num_classes=10, hidden_dims=[512, 256, 128], dropout=0.2, activation_fn=activation_fn)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer Selection\n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR every 5 epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(images.shape[0], -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Improved Model (LR Scheduler, 15 Epochs)\n"
     ]
    }
   ],
   "source": [
    "# Train the improved model\n",
    "print(\"Training Improved Model (LR Scheduler, 15 Epochs)\")\n",
    "activation_fn = nn.ReLU()\n",
    "optimizer = \"Adam\"\n",
    "\n",
    "improved_model = train_with_scheduler(activation_fn, optimizer, num_epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
